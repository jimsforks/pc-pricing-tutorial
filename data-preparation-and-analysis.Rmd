# Data Preparation and Analysis

Perhaps the most difficult aspect of the predictive modeling workflow to write about is data preparation. When it comes to predictive modeling on structured data, the techniques and their implementations are more or less standard across industries. On the other hand, there is tremendous variability in what data sources look like at insurers and therefore in the paths to arrive at modeling-ready datasets from raw data (not to mention navigating politics to acquire said data!). However, we are talking about insurance after all, so the concept of policies, exposures, and claims are relativel invariant across companies.

In this chapter, we introduce the datasets we'll be using throughout the book, walk through data preparation, and perform exploratory data analysis.

## The AUTOSEG Dataset {#autoseg}

We use publicly available data from AUTOSEG ("Automobile Statistics System") from Brazil's Superintendence of Private Insurance (SUSEP). It maintains policy-characteristics-level data for personal auto from 2007 through the present for all insured vehicles in Brazil. The data contains a variety of variables, from policyholder characteristics to losses by peril. It also contains a set of mapping tables that define the codes used in categorical variables, which allows us to demonstrate a range data manipulation actions such as joining. The data can be downloaded directly from SUSEP's [AUTOSEG](http://www2.susep.gov.br/menuestatistica/Autoseg/principal.aspx) website, but we also host it on the book's [GitHub repository](https://github.com/kasaai/pc-pricing-tutorial) for covenience.

The raw data is organized in Zip archives containing half a year worth of data each. Tables \@ref(tab:main-tables) and \@ref(tab:aux-tables) list the data files and their descriptions included in each bundle.

Table: (\#tab:main-tables) Main Tables

| File      | Description                                                          |
|-----------|----------------------------------------------------------------------|
| arq_casco | Exposure data, premiums, claims and insured amount for the CASCO overhead, classified by the Key Category Rate / Region / Model / Year / Sex / Age Range |
| arq_casco3| Exposure data, premiums and claims for the CASCO overhang, classified by the Key Rate Category / CEP / Model / Year key |
|arq_casco4 | Exposure data, premiums and claims for the CASCO overhang, classified by the Key Rate Category / City / Model / Year |
| premreg   | Regional distribution of prices |
| sinreg    | Regional distribution of claims |

Table: (\#tab:aux-tables) Mapping (Auxiliary) Tables

| File        | Description                                                          |
|-------------|----------------------------------------------------------------------|
| auto2_vei   | FIPE code and description of each vehicle model, in addition to the group code to which it belongs |
| auto2_group | Code and description of model groups it contains                     |
| auto_cat    | Description code of tariff categories                                |
| auto_cau    | Code and description of causes of accidents                          |
| auto_cep    | Correlates the CEP with cities and regions of circulation            |
| auto_cob    | Code and description of covers                                       |
| auto_idade  | Code and description of age groups                                   |
| auto_reg    | Code and description of regions of circulation                       |
| auto_sexo   | Code and description of sex (male, female, legal)                    |
| auto_city   | Code and name of cities                                              |


The excerpts for each of the source tables, before any transformations, can be found in Appendix \@ref(raw-table-excerpts). (TODO: maybe we should include translations here.)

For our purposes, we will use the `arq_casco` policy table since it is the only one that contains poilcyholder characteristics -- specifically, sex and age.

## The Data Model

We see that there are a few categorical columns in `arq_casco` with codes that we need to decipher using the provided mapping tables. To accomplish this, we first observe the  relational model diagram in Figure \@ref(fig:data-model).

```{r, data-model, echo = FALSE, message=FALSE, fig.cap="Relational model of AUTOSEG tables."}
library(datamodelr)
library(drake)
library(zeallot)
create_data_graph <- function(raw_data) {
  c(auto_cat, auto_cau, auto_cep, auto_cidade,
    auto_cob, auto_idade, auto_reg, auto_sexo,
    auto2_grupo, auto2_vei, risks_table) %<-% raw_data
  
  dm <- dm_from_data_frames(
    risks_table, auto_cat,
    auto_cau, auto_idade, auto_reg,
    auto_sexo, auto2_vei)
  
  dm <- dm_add_references(
    dm,
    risks_table$vehicle_category_code == auto_cat$vehicle_category_code,
    risks_table$sex_code == auto_sexo$sex_code,
    risks_table$region_code == auto_reg$region_code,
    risks_table$age_code == auto_idade$age_code,
    risks_table$vehicle_code == auto2_vei$vehicle_code
  )
  dm_create_graph(dm)
}

graph <- raw_data %>% 
  create_data_graph()
datamodelr::dm_render_graph(graph)
```

The table `auto_cau` stands out from the others because it defines the numerical codes used in the claim count and claim amount columns, so we use it to rename those columns in `risks_table` rather than joining them. For example, the column `claim_count1` becomes `claim_count_theft`.

For the rest of the mapping tables, we join them on to the main `risks_table` by the associated keys. As an example, for the table `auto_idade`, which we exhibit in Table \@ref(tab:auto-idade), we join on the column `age_code`. The type of join we perform is a *left join*, which means we retain all rows of `risks_table`, and append the matched rows in `auto_idade`, so that the row with `age_code == "0"` will have `age == "Not provided"`, the row with `age_code == "1"` will have `age == "18 to 25"`, and so on.

```{r auto-idade, echo = FALSE}
knitr::kable(raw_data$auto_idade, booktabs = TRUE, caption = "auto_idade.")
```

## Training/Testing Split

Now that we have acquired the data we will use for modeling, the immediate next step is to set aside a holdout, or testing, set. The purpose of the testing set is to serve as a way for us to validate our model at the end of the modeling exercise. It is crucial that we do not peek at the testing set during exploratory data analysis or model tuning, otherwise we may allow information from the testing set to "leak" to the training phase, which undermines the credibility of our performance metrics.

We are going to set aside a random 1/5 of the available data as our testing set, and lock it up in a drawer (figuratively) and not look at it until the very end. (TODO: maybe consider time split when we expand to more years.) Until we say otherwise, the following analyses will all be done on the training set.

## Data Summary

Perhaps the first thing to look at is a brief summary of our combined dataset to get an idea about types, missingness, and distributions. In Tables \@ref(tab:data-summary-character) and \@ref(tab:data-summary-numeric) we have the summaries of the character and numeric columns, respectively.

```{r echo = FALSE, message=FALSE}
library(skimr)
library(kableExtra)
library(gt)
summary_list <- training_data %>% skimr::skim_to_list()
```

```{r data-summary-character, echo = FALSE}
summary_list$character %>%
  gt::gt() %>%
  # TODO: this is a hack before https://github.com/rstudio/gt/pull/143
  #   is resolved
  gt::tab_header(html("Summary of character columns of training dataset. <span id='tab:data-summary-character'></span>"))
```

```{r data-summary-numeric, echo = FALSE}
rbind(summary_list$integer, summary_list$numeric) %>% 
  gt::gt() %>%
  gt::tab_header(html("Summary of numeric columns of training dataset.<span id='tab:data-summary-numeric'></span>"))
```

## Exposures by state

(something like this, will need to add tooltip and cut out Brazil)

```{r, fig.height = 6}
# brazil %>%
#   leaflet(options = leafletOptions(minZoom = 4)) %>%
#   addTiles() %>%
#   addPolygons(
#     data = bra_cutout,
#     weight  = 0,
#     opacity = 1,
#     color = "white",
#     fillOpacity = 1
#   ) %>%
#   addPolygons(
#     fillColor = ~pal(exposures),
#     weight = 2,
#     opacity = 1,
#     color = "white",
#     dashArray = "3",
#     fillOpacity = 0.7,
#     label = lapply(labels, htmltools::HTML),
#     highlight = highlightOptions(
#       weight = 5,
#       color = "#666",
#       dashArray = "",
#       fillOpacity = 0.7,
#       bringToFront = TRUE)
#   ) %>%
#   setMaxBounds(bra_bbox[1], bra_bbox[2], bra_bbox[3], bra_bbox[4]) %>%
#   setView(mean(bra_bbox[c(1,3)]), mean(bra_bbox[c(2,4)]), zoom = 4)
```
<!-- The `.gpkg` files for drawing state boundaries can be downloaded from [https://biogeo.ucdavis.edu/data/gadm3.6/gpkg/gadm36_BRA_gpkg.zip](https://biogeo.ucdavis.edu/data/gadm3.6/gpkg/gadm36_BRA_gpkg.zip). See [https://gadm.org/download_country_v3.html](https://gadm.org/download_country_v3.html) for details. -->